diff --git a/docs/micro-controllers/basic_usage.rst b/docs/micro-controllers/basic_usage.rst
index d1d955e923a3f2c1d3d3bd0bae457b304997d7e0..879a8d35b657b2f0e5c383bc78a3fb452ccefc01 100644
--- a/docs/micro-controllers/basic_usage.rst
+++ b/docs/micro-controllers/basic_usage.rst
@@ -1,8 +1,8 @@
 Basic usage for Micro Controllers
 ==================================
 
-MACE Micro is a lightweight neural network inference engine for MCUs and low-power DSPs.
-At now we support Cortex-M MCUs and Qualcomm Hexagon DSPs. You can get our projects from GitHub.
+MACE Micro is a lightweight neural network inference engine for MCUs.
+At now we support Cortex-M MCUs. You can get our projects from GitHub.
 
 Get MACE Micro Projects
 -----------------------
diff --git a/docs/micro-controllers/deploy.rst b/docs/micro-controllers/deploy.rst
index fa480beb472fc0872cd80bb047ec7c4b5b7556dd..d6dced7f230f2c3b81eb8ac30de83bcc1b2f9f5e 100644
--- a/docs/micro-controllers/deploy.rst
+++ b/docs/micro-controllers/deploy.rst
@@ -41,8 +41,3 @@ You can use the Mace Micro install package("build/micro/gcc-arm-none-eabi/instal
     mbed sterm
 
 Press the reset(black) button to run the example again.
-
-For Hexagon DSP
----------------
-
-In the micro/cmake/toolchain folder, there are two hexagon CMake toolchain files for reference, For more details, please goto <https://developer.qualcomm.com/software/hexagon-dsp-sdk/dsp-processor/>
\ No newline at end of file
diff --git a/mace/core/runtime/hexagon/hexagon_dsp_wrapper.cc b/mace/core/runtime/hexagon/hexagon_dsp_wrapper.cc
index 87a6800e83ad3bb9dc1385467422ac225a938899..969036a209711075f6884d6b6543c6be3e23bedc 100644
--- a/mace/core/runtime/hexagon/hexagon_dsp_wrapper.cc
+++ b/mace/core/runtime/hexagon/hexagon_dsp_wrapper.cc
@@ -564,12 +564,6 @@ bool HexagonDSPWrapper::ExecuteGraphNew(
     MACE_CHECK(output_shape.size() == output_info_[i].shape.size(),
                output_shape.size(), " vs ", output_info_[i].shape.size(),
                 " wrong output shape inferred");
-    for (size_t j = 0; j < output_shape.size(); ++j) {
-      MACE_CHECK(static_cast<index_t>(output_shape[j])
-                     == output_info_[i].shape[j],
-                 output_shape[j], " vs ", output_info_[i].shape[j],
-                 " wrong output shape[", j, "] inferred");
-    }
     auto output_tensor = output_tensors->at(output_info_[i].name);
     MACE_CHECK(static_cast<index_t>(outputs[index].data_valid_len)
                     == output_tensor->raw_size(),
diff --git a/mace/libmace/mace.cc b/mace/libmace/mace.cc
index 77e1daa7453388b6b6b09872e600c9295d6093d3..394cbea79f174e4e0c19cededc152a78bd056fba 100644
--- a/mace/libmace/mace.cc
+++ b/mace/libmace/mace.cc
@@ -25,6 +25,7 @@
 #include "mace/core/registry/op_delegator_registry.h"
 #include "mace/ops/common/transpose.h"
 #include "mace/ops/registry/registry.h"
+#include "mace/utils/conf_util.h"
 #include "mace/utils/math.h"
 #include "mace/utils/memory.h"
 #include "mace/utils/stl_util.h"
@@ -793,7 +794,7 @@ MaceEngine::Impl::~Impl() {
     if (VLOG_IS_ON(2)) {
       hexagon_controller_->PrintLog();
     }
-    if (VLOG_IS_ON(1)) {
+    if (EnvConfEnabled("MACE_HEXAGON_PROFILING")) {
       hexagon_controller_->GetPerfInfo();
     }
     MACE_CHECK(hexagon_controller_->TeardownGraph(), "hexagon teardown error");
@@ -943,9 +944,6 @@ MaceStatus MaceEngine::Impl::TransposeOutput(
                    << static_cast<int>(output->second.data_format()) << " vs "
                    << static_cast<int>(output_tensor->data_format());
       }
-      VLOG(1) << "Transform output " << output->first << " from "
-              << static_cast<int>(output_tensor->data_format()) << " to "
-              << static_cast<int>(output->second.data_format());
       std::vector<index_t> shape =
           TransposeShape<index_t, index_t>(output_tensor->shape(),
                                            dst_dims);
diff --git a/mace/tools/mace_run.cc b/mace/tools/mace_run.cc
index f559555a61f5db28e44ff9d9986cabeefbc83938..5206f5998d68999a9ee5f18408a8e23c66f932f2 100644
--- a/mace/tools/mace_run.cc
+++ b/mace/tools/mace_run.cc
@@ -171,6 +171,11 @@ bool RunModel(const std::string &model_name,
               const std::vector<DataFormat> &output_data_formats,
               float cpu_capability) {
   DeviceType device_type = ParseDeviceType(FLAGS_device);
+  bool mace_benchmark_op = false;
+  if (FLAGS_benchmark &&
+      (device_type == DeviceType::CPU || device_type == DeviceType::GPU)) {
+    mace_benchmark_op = true;
+  }
 
   int64_t t0 = NowMicros();
   // config runtime
@@ -448,7 +453,7 @@ bool RunModel(const std::string &model_name,
         MaceStatus run_status;
         RunMetadata metadata;
         RunMetadata *metadata_ptr = nullptr;
-        if (FLAGS_benchmark) {
+        if (mace_benchmark_op) {
           metadata_ptr = &metadata;
         }
 
@@ -488,7 +493,7 @@ bool RunModel(const std::string &model_name,
           } else {
             int64_t t1 = NowMicros();
             total_run_duration += (t1 - t0);
-            if (FLAGS_benchmark) {
+            if (mace_benchmark_op) {
               op_stat.StatMetadata(metadata);
             }
             break;
@@ -521,7 +526,7 @@ bool RunModel(const std::string &model_name,
     printf("========================================================\n");
     printf("time %15.3f %11.3f %11.3f %11.3f\n",
            cpu_capability, init_millis, warmup_millis, model_run_millis);
-    if (FLAGS_benchmark) {
+    if (mace_benchmark_op) {
       op_stat.PrintStat();
     }
   }
@@ -545,6 +550,7 @@ int Main(int argc, char **argv) {
 
   if (FLAGS_benchmark) {
     setenv("MACE_OPENCL_PROFILING", "1", 1);
+    setenv("MACE_HEXAGON_PROFILING", "1", 1);
   }
 
   LOG(INFO) << "model name: " << FLAGS_model_name;
diff --git a/tools/device.py b/tools/device.py
index 6ab0135bc64ae6c23359b1c89c3a375ea60da7d1..5d39b807a5c1c047d5b0a6e1d32cb220ecdfa145 100644
--- a/tools/device.py
+++ b/tools/device.py
@@ -686,7 +686,9 @@ class DeviceWrapper:
             model_path = "%s/%s.pb" % (mace_model_dir, model_name)
             output_config = {YAMLKeyword.model_file_path: model_path,
                              YAMLKeyword.output_tensors: output_nodes,
-                             YAMLKeyword.output_shapes: output_shapes}
+                             YAMLKeyword.output_shapes: output_shapes,
+                             YAMLKeyword.output_data_formats:
+                                 subgraphs[0][YAMLKeyword.output_data_formats]}
             output_configs.append(output_config)
 
             runtime_list = []
@@ -797,7 +799,7 @@ class DeviceWrapper:
                                     YAMLKeyword.output_shapes],
                                 input_data_formats=subgraphs[0][
                                     YAMLKeyword.input_data_formats],
-                                output_data_formats=subgraphs[0][
+                                output_data_formats=output_config[
                                     YAMLKeyword.output_data_formats],
                                 model_output_dir=model_output_dir,
                                 input_data_types=subgraphs[0][
diff --git a/tools/layers_validate.py b/tools/layers_validate.py
index b6a63b85b284c14bc84487ed3e19483cac493fc8..e6d916ab70f7d0e34b88b6f0d0f01cd1775c7438 100644
--- a/tools/layers_validate.py
+++ b/tools/layers_validate.py
@@ -19,6 +19,7 @@ import sys
 import yaml
 
 sys.path.insert(0, "tools/python")  # noqa
+from common import DataFormat
 from py_proto import mace_pb2
 from transform.base_converter import ConverterUtil
 from transform.base_converter import MaceKeyword
@@ -132,8 +133,9 @@ def convert(model_file, output_dir, layers):
         if is_quantize:
             op.name = MaceKeyword.mace_output_node_name + '_' + op.name
         if is_hexagon:
-            mace_check(len(op.output) == 1,
-                       "Only supports number of outputs of Hexagon op be 1.")
+            if len(op.output) != 1:
+                print("Skip %s(%s)" % (op.name, op.type))
+                continue
         for i in range(len(op.output)):
             output_tensors.append(str(op.output[i]))
             output_shapes.append(
@@ -187,7 +189,8 @@ def convert(model_file, output_dir, layers):
                                          output_dir)
         output_config = {"model_file_path": str(model_path),
                          "output_tensors": output_tensors,
-                         "output_shapes": output_shapes}
+                         "output_shapes": output_shapes,
+                         "output_data_formats": [common.DataFormat.NHWC]}
         output_configs["subgraphs"].append(output_config)
 
     output_configs_path = output_dir + "outputs.yml"
diff --git a/tools/python/layers_validate.py b/tools/python/layers_validate.py
index 2aab64aff5fe1b6e2acbca813950bbe24535da84..0eaa7cb692328a1a82a9a5f5b67ee2ebf1b857e8 100644
--- a/tools/python/layers_validate.py
+++ b/tools/python/layers_validate.py
@@ -23,6 +23,7 @@ from transform.base_converter import ConverterUtil
 from transform.base_converter import MaceKeyword
 from transform.base_converter import MaceOp
 from transform.hexagon_converter import HexagonOp
+from utils.config_parser import DataFormat
 from utils.util import mace_check
 
 
@@ -128,8 +129,9 @@ def convert(model_file, output_dir, layers):
         if is_quantize:
             op.name = MaceKeyword.mace_output_node_name + '_' + op.name
         if is_hexagon:
-            mace_check(len(op.output) == 1,
-                       "Only supports number of outputs of Hexagon op be 1.")
+            if len(op.output) != 1:
+                print("Skip %s(%s)" % (op.name, op.type))
+                continue
         for i in range(len(op.output)):
             output_tensors.append(str(op.output[i]))
             output_shapes.append(
@@ -183,7 +185,8 @@ def convert(model_file, output_dir, layers):
                                          output_dir)
         output_config = {"model_file_path": str(model_path),
                          "output_tensors": output_tensors,
-                         "output_shapes": output_shapes}
+                         "output_shapes": output_shapes,
+                         "output_data_formats": [DataFormat.NHWC]}
         output_configs["subgraphs"].append(output_config)
 
     output_configs_path = output_dir + "outputs.yml"
diff --git a/tools/python/transform/hexagon_converter.py b/tools/python/transform/hexagon_converter.py
index 29a82e49580af6264f31907e14adfdb7453792bb..923e8a83ef6c8dc5c4b67eb334c5e93260b365b5 100644
--- a/tools/python/transform/hexagon_converter.py
+++ b/tools/python/transform/hexagon_converter.py
@@ -29,12 +29,14 @@ from transform.base_converter import ActivationType
 from transform.base_converter import ConverterUtil
 from transform.base_converter import DeviceType
 from transform.base_converter import EltwiseType
+from transform.base_converter import FrameworkType
 from transform.base_converter import MaceKeyword
 from transform.base_converter import MaceOp
 from transform.base_converter import PaddingMode
 from transform.base_converter import PadType
 from transform.base_converter import PoolingType
 from transform.base_converter import ReduceType
+from quantize import quantize_util
 from utils.util import mace_check
 
 
@@ -47,23 +49,28 @@ HexagonSupportedOps = [
     'OUTPUT',
     'QuantizedAdd_8p8to8',
     'QuantizedAvgPool_8',
+    'QuantizedBatchNorm_8x8p8to8',
     'QuantizedConcat_8',
+    'QuantizedDiv_8',
     'QuantizedMaxPool_8',
     'QuantizedMaximum_8',
     'QuantizedMinimum_8',
     'QuantizedMul_8x8to8',
     'QuantizedPad_8',
+    'QuantizedRecip_8',
     'QuantizedRelu_8',
     'QuantizedReluX_8',
     'QuantizedReshape',
     'QuantizedResizeBilinear_8',
     'QuantizedSigmoid_8',
     'QuantizedSoftmax_8',
+    'QuantizedSplit_8',
     'QuantizedStridedSlice_8',
     'QuantizedSub_8p8to8',
     'QuantizedTanh_8',
     'QuantizedTransposeConv2d_8x8p32to8',
     'QuantizeINPUT_f_to_8',
+    'ResizeNearestNeighbor_8',
     'SpaceToBatchND_8',
     'SpaceToDepth_8',
     'Supernode_8x8p32to8',
@@ -94,21 +101,18 @@ def get_op_and_port_from_tensor(tensor_name):
     return op, port
 
 
-def get_input_shape(tensor_name, model):
-    producer_op_name, _ = get_op_and_port_from_tensor(tensor_name)
-    input_shape = None
-    for producer_op in model.op:
-        if producer_op.name == producer_op_name:
-            input_shape = producer_op.output_shape[0].dims
-            break
-    mace_check(input_shape is not None, "Missing input shape.")
-    return input_shape
-
-
 def normalize_name(name):
     return name.replace(':', '_')
 
 
+def add_port_for_tensor(name):
+    return name + ':0' if ':' not in name else name
+
+
+def remove_port_for_tensor(name):
+    return name[:-2] if ':0' in name else name
+
+
 class HexagonConverter(base_converter.ConverterInterface):
     activation_type = {
         ActivationType.RELU.name: HexagonOp.QuantizedRelu_8.name,
@@ -123,15 +127,18 @@ class HexagonConverter(base_converter.ConverterInterface):
         EltwiseType.PROD.value: HexagonOp.QuantizedMul_8x8to8.name,
         EltwiseType.MIN.value: HexagonOp.QuantizedMinimum_8.name,
         EltwiseType.MAX.value: HexagonOp.QuantizedMaximum_8.name,
+        EltwiseType.DIV.value: HexagonOp.QuantizedDiv_8.name,
     }
 
     def __init__(self, option, model, quantize_activation_info):
         self._option = option
         self._model = model
         self._consts = {}
+        self._producers = {}
         self._quantize_activation_info = quantize_activation_info
         self._op_converters = {
             MaceOp.Activation.name: self.convert_activation,
+            MaceOp.BatchNorm.name: self.convert_batchnorm,
             MaceOp.BatchToSpaceND.name: self.convert_batchspace,
             MaceOp.Concat.name: self.convert_concat,
             MaceOp.Conv2D.name: self.convert_conv2d,
@@ -146,15 +153,19 @@ class HexagonConverter(base_converter.ConverterInterface):
             MaceOp.Quantize.name: self.convert_quantize,
             MaceOp.Reduce.name: self.convert_reduce,
             MaceOp.ResizeBilinear.name: self.convert_resizebilinear,
+            MaceOp.ResizeNearestNeighbor.name:
+                self.convert_resizenearestneighbor,
             MaceOp.Softmax.name: self.convert_softmax,
+            MaceOp.Split.name: self.convert_split,
             MaceOp.StridedSlice.name: self.convert_stridedslice,
             MaceOp.SpaceToBatchND.name: self.convert_batchspace,
             MaceOp.SpaceToDepth.name: self.convert_depthspace,
         }
+        self._framework_type = ConverterUtil.get_arg(
+            self._model, MaceKeyword.mace_framework_type_str).i
 
     def run(self):
-        for tensor in self._model.tensors:
-            self._consts[tensor.name] = tensor
+        self.add_port_and_construct_producers()
 
         # convert op node
         self.convert_ops()
@@ -163,8 +174,34 @@ class HexagonConverter(base_converter.ConverterInterface):
 
         self.add_node_id(model_inputs)
 
+        self.remove_port()
+
         return self._model
 
+    def add_port_and_construct_producers(self):
+        for tensor in self._model.tensors:
+            tensor.name = add_port_for_tensor(tensor.name)
+            self._consts[tensor.name] = tensor
+        for key in tuple(self._quantize_activation_info):
+            name = add_port_for_tensor(key)
+            self._quantize_activation_info[name] = \
+                self._quantize_activation_info[key]
+        for op in self._model.op:
+            for i in range(len(op.output)):
+                op.output[i] = add_port_for_tensor(op.output[i])
+                if op.output[i] not in self._producers:
+                    self._producers[op.output[i]] = op
+
+    def get_input_shape(self, tensor_name):
+        mace_check(tensor_name in self._producers, "Missing producer.")
+        op = self._producers[tensor_name]
+        input_shape = None
+        for i, output in enumerate(op.output):
+            if output == tensor_name:
+                input_shape = op.output_shape[i].dims
+        mace_check(input_shape is not None, "Missing input shape.")
+        return input_shape
+
     def add_port_for_tensors(self,  tensors):
         for i in range(len(tensors)):
             if ':' not in tensors[i]:
@@ -174,6 +211,33 @@ class HexagonConverter(base_converter.ConverterInterface):
                     self._quantize_activation_info[tensors[i]] = \
                         self._quantize_activation_info[node_name]
 
+    def remove_port(self):
+        if self._framework_type == FrameworkType.TENSORFLOW.value:
+            return
+        for op in self._model.op:
+            for i in range(len(op.input)):
+                op.input[i] = remove_port_for_tensor(op.input[i])
+            for i in range(len(op.output)):
+                op.output[i] = remove_port_for_tensor(op.output[i])
+        for tensor in self._model.tensors:
+            tensor.name = remove_port_for_tensor(tensor.name)
+
+    def add_quantized_scalar_const_node(self, name, val, op=None):
+        if op is not None:
+            name = op.name + name
+            op.input.append(name)
+        if name not in self._consts:
+            quantized_tensor = quantize_util.quantize(
+                [val], DeviceType.HEXAGON.value, False)
+            tensor = self._model.tensors.add()
+            self._consts[name] = tensor
+            tensor.name = name
+            tensor.data_type = mace_pb2.DT_UINT8
+            tensor.dims.extend([1])
+            tensor.int32_data.extend(quantized_tensor.data)
+            tensor.minval = quantized_tensor.minval
+            tensor.maxval = quantized_tensor.maxval
+
     def add_scalar_const_node(self, name, val, op=None):
         if op is not None:
             name = op.name + name
@@ -198,9 +262,18 @@ class HexagonConverter(base_converter.ConverterInterface):
         else:
             op.input.insert(insert_index, arg_tensor.name)
 
+    def add_min_max_const_node_for_split(self, this_op, tensor_name):
+        op, port = get_op_and_port_from_tensor(tensor_name)
+        this_op.input.extend([op + ':2'])
+        this_op.input.extend([op + ':3'])
+
     def add_min_max_const_node(
             self, this_op, tensor_name, add_min=True, add_max=True,
             diff_port=True):
+        if tensor_name in self._producers and \
+                self._producers[tensor_name].type == \
+                HexagonOp.QuantizedSplit_8.name:
+            return self.add_min_max_const_node_for_split(this_op, tensor_name)
         op, port = get_op_and_port_from_tensor(tensor_name)
         mace_check(port == 0, 'port should be 0 to add min max tensor then.')
         if tensor_name in self._quantize_activation_info:
@@ -357,6 +430,38 @@ class HexagonConverter(base_converter.ConverterInterface):
                             port += i * 3
                 node_input.output_port = port
 
+    def add_bias(self, op):
+        print('Hexagon conv/deconv requires biasadd, we add it.')
+        channels = op.output_shape[0].dims[3]
+        bias_data = np.zeros(channels, dtype=int)
+        bias_tensor = self._model.tensors.add()
+        bias_tensor.data_type = mace_pb2.DT_INT32
+        bias_tensor.dims.extend([channels])
+        bias_tensor.int32_data.extend(bias_data)
+        bias_tensor.minval = 0
+        bias_tensor.maxval = 0
+        bias_tensor.name = op.name + "/bias:0"
+        bias = bias_tensor.name
+        self._consts[bias] = bias_tensor
+        return bias
+
+    def add_padding_type_for_conv_pooling(self, op, kernels, strides):
+        input_shape = self.get_input_shape(op.input[0])
+        output_shape = op.output_shape[0].dims
+        in_h, in_w = input_shape[1], input_shape[2]
+        k_h, k_w = kernels[0], kernels[1]
+        out_h, out_w = output_shape[1], output_shape[2]
+
+        if (out_h == (in_h - k_h) // strides[0] + 1) and \
+                (out_w == (in_w - k_w) // strides[1] + 1):
+            op.padding = padding_mode[PaddingMode.VALID]
+        elif (out_h == (in_h - 1) // strides[0] + 1) and \
+                (out_w == (in_w - 1) // strides[1] + 1):
+            op.padding = padding_mode[PaddingMode.SAME]
+        else:
+            mace_check(False,
+                       "Hexagon does not support padding type for: %s" % op)
+
     def convert_ops(self):
         print("Convert mace graph to hexagon.")
         for op in self._model.op:
@@ -385,10 +490,11 @@ class HexagonConverter(base_converter.ConverterInterface):
                 out_max_byte_size *= 4
             op.out_max_byte_size.extend([out_max_byte_size])
 
-        op.padding = padding_mode[PaddingMode.NA]
-        arg = ConverterUtil.get_arg(op, MaceKeyword.mace_padding_str)
-        if arg is not None:
-            op.padding = padding_mode[PaddingMode(arg.i)]
+        if not op.HasField("padding"):
+            op.padding = padding_mode[PaddingMode.NA]
+            arg = ConverterUtil.get_arg(op, MaceKeyword.mace_padding_str)
+            if arg is not None:
+                op.padding = padding_mode[PaddingMode(arg.i)]
 
     def convert_activation(self, op):
         self.add_min_max_const_node(op, op.input[0])
@@ -405,6 +511,17 @@ class HexagonConverter(base_converter.ConverterInterface):
             mace_check(False,
                        "Hexagon does not support activation %s" % act_type)
 
+    def convert_batchnorm(self, op):
+        bias = op.input.pop()
+        self.add_min_max_const_node(op, op.input[0])
+        self.add_min_max_const_node(op, op.input[1])
+        op.input.append(bias)
+        self.add_min_max_const_node(op, bias)
+        self.add_min_max_const_node(
+            op, op.output[0], True, True, False)
+
+        op.type = HexagonOp.QuantizedBatchNorm_8x8p8to8.name
+
     def convert_batchspace(self, op):
         strides_arg = ConverterUtil.get_arg(
             op, MaceKeyword.mace_space_batch_block_shape_str)
@@ -442,19 +559,8 @@ class HexagonConverter(base_converter.ConverterInterface):
         op.type = HexagonOp.QuantizedConcat_8.name
 
     def convert_conv2d(self, op):
-        channels = op.output_shape[0].dims[3]
         if len(op.input) < 3:
-            print('Supernode requires biasadd, we add it.')
-            bias_data = np.zeros(channels, dtype=int)
-            bias_tensor = self._model.tensors.add()
-            bias_tensor.data_type = mace_pb2.DT_INT32
-            bias_tensor.dims.extend([channels])
-            bias_tensor.int32_data.extend(bias_data)
-            bias_tensor.minval = 0
-            bias_tensor.maxval = 0
-            bias_tensor.name = op.name + "/bias:0"
-            bias = bias_tensor.name
-            self._consts[bias] = bias_tensor
+            bias = self.add_bias(op)
         else:
             bias = op.input.pop()
 
@@ -472,6 +578,14 @@ class HexagonConverter(base_converter.ConverterInterface):
         self.add_min_max_const_node(
             op, op.output[0], True, True, False)
 
+        self.add_padding_type_for_conv_pooling(
+            op, self._consts[op.input[1]].dims, strides_arg.ints)
+
+        dilations_arg = ConverterUtil.get_arg(op, 'dilations')
+        mace_check(dilations_arg is None or
+                   (dilations_arg.ints[0] == 1 and dilations_arg.ints[1] == 1),
+                   "Hexagon only support dilations[1,1].")
+
         if op.type == MaceOp.DepthwiseConv2d.name:
             op.type = HexagonOp.DepthwiseSupernode_8x8p32to8.name
         else:
@@ -480,57 +594,63 @@ class HexagonConverter(base_converter.ConverterInterface):
     def add_deconv_pad_node(self, op):
         padding_type_arg = \
             ConverterUtil.get_arg(op, MaceKeyword.mace_padding_type_str)
-        mace_check(padding_type_arg is not None, "Missing padding of Deconv.")
-        padding_type = PaddingMode(padding_type_arg.i)
-        strides_arg = ConverterUtil.get_arg(op, MaceKeyword.mace_strides_str)
-        mace_check(strides_arg is not None, "Missing strides of Deconv.")
-        stride_h = strides_arg.ints[0]
-        stride_w = strides_arg.ints[1]
-
-        input_shape = get_input_shape(op.input[0], self._model)
-        input_h = input_shape[1]
-        input_w = input_shape[2]
-        filter_tensor = self._consts[op.input[1]]
-        filter_h = filter_tensor.dims[1]
-        filter_w = filter_tensor.dims[2]
-        output_h = op.output_shape[0].dims[1]
-        output_w = op.output_shape[0].dims[2]
-
-        if padding_type == PaddingMode.VALID:
-            expected_input_h = (output_h - filter_h + stride_h) // stride_h
-            expected_input_w = (output_w - filter_w + stride_w) // stride_w
-        elif padding_type == PaddingMode.SAME:
-            expected_input_h = (output_h + stride_h - 1) // stride_h
-            expected_input_w = (output_w + stride_w - 1) // stride_w
+        padding_values_arg = \
+            ConverterUtil.get_arg(op, MaceKeyword.mace_padding_values_str)
+        mace_check(padding_type_arg is not None or
+                   padding_values_arg is not None,
+                   "Missing padding of Deconv.")
+        if padding_type_arg is not None:
+            padding_type = PaddingMode(padding_type_arg.i)
+            strides_arg = ConverterUtil.get_arg(op,
+                                                MaceKeyword.mace_strides_str)
+            mace_check(strides_arg is not None, "Missing strides of Deconv.")
+            stride_h = strides_arg.ints[0]
+            stride_w = strides_arg.ints[1]
+
+            input_shape = self.get_input_shape(op.input[0])
+            input_h = input_shape[1]
+            input_w = input_shape[2]
+            filter_tensor = self._consts[op.input[1]]
+            filter_h = filter_tensor.dims[1]
+            filter_w = filter_tensor.dims[2]
+            output_h = op.output_shape[0].dims[1]
+            output_w = op.output_shape[0].dims[2]
+
+            if padding_type == PaddingMode.VALID:
+                expected_input_h = (output_h - filter_h + stride_h) // stride_h
+                expected_input_w = (output_w - filter_w + stride_w) // stride_w
+            elif padding_type == PaddingMode.SAME:
+                expected_input_h = (output_h + stride_h - 1) // stride_h
+                expected_input_w = (output_w + stride_w - 1) // stride_w
+            else:
+                raise Exception(
+                    'Hexagon deconv does not support padding type: ',
+                    padding_type)
+            mace_check(expected_input_h == input_h,
+                       "Wrong input/output height")
+            mace_check(expected_input_w == input_w, "Wrong input/output width")
+
+            pad_h = (input_h - 1) * stride_h + filter_h - output_h
+            pad_w = (input_w - 1) * stride_w + filter_w - output_w
         else:
-            raise Exception('Hexagon deconv does not support padding type: ',
-                            padding_type)
-        mace_check(expected_input_h == input_h, "Wrong input/output height")
-        mace_check(expected_input_w == input_w, "Wrong input/output width")
-
-        pad_h = (input_h - 1) * stride_h + filter_h - output_h
-        pad_w = (input_w - 1) * stride_w + filter_w - output_w
+            pad_h = padding_values_arg.ints[0]
+            pad_w = padding_values_arg.ints[1]
         pad_h, pad_w = max(pad_h, 0), max(pad_w, 0)
         paddings = [pad_h, pad_h, pad_w, pad_w]
         self.add_arg_const_node(op, "/paddings:0", [1, 1, 2, 2], paddings)
 
     def convert_deconv2d(self, op):
-        channels = op.output_shape[0].dims[3]
-        if len(op.input) < 4:
-            print('Hexagon deconv requires biasadd, we add it.')
-            bias_data = np.zeros(channels, dtype=int)
-            bias_tensor = self._model.tensors.add()
-            bias_tensor.data_type = mace_pb2.DT_INT32
-            bias_tensor.dims.extend([channels])
-            bias_tensor.int32_data.extend(bias_data)
-            bias_tensor.minval = 0
-            bias_tensor.maxval = 0
-            bias_tensor.name = op.name + "/bias:0"
-            bias = bias_tensor.name
-            self._consts[bias] = bias_tensor
+        if self._framework_type == FrameworkType.TENSORFLOW.value:
+            if len(op.input) < 4:
+                bias = self.add_bias(op)
+            else:
+                bias = op.input.pop()
+            op.input.pop()  # output shape
         else:
-            bias = op.input.pop()
-        op.input.pop()  # output shape
+            if len(op.input) < 3:
+                bias = self.add_bias(op)
+            else:
+                bias = op.input.pop()
 
         self.add_min_max_const_node(op, op.input[0])
         self.add_min_max_const_node(op, op.input[1])
@@ -567,19 +687,34 @@ class HexagonConverter(base_converter.ConverterInterface):
         op.type = HexagonOp.DequantizeOUTPUT_8tof.name
 
     def convert_elementwise(self, op):
+        element_type = ConverterUtil.get_arg(
+            op, MaceKeyword.mace_element_type_str).i
+
+        if element_type == EltwiseType.DIV.value and \
+                op.input[0] in self._consts:
+            tensor = self._consts[op.input[0]]
+            if len(tensor.int32_data) == 1:
+                f = tensor.scale * (tensor.int32_data[0] - tensor.zero_point)
+                if abs(f - 1) < 1e-6:  # recip
+                    op_input = op.input[1]
+                    del op.input[:]
+                    op.input.append(op_input)
+                    self.add_min_max_const_node(op, op.input[0])
+                    op.type = HexagonOp.QuantizedRecip_8.name
+                    return
+
         if len(op.input) == 1:
-            scalar_input_arg = ConverterUtil.get_arg(
-                op, MaceKeyword.mace_scalar_input_str)
-            self.add_scalar_const_node("/b:0", scalar_input_arg.f, op)
+            scalar_input = ConverterUtil.get_arg(
+                op, MaceKeyword.mace_scalar_input_str).f
+            self.add_quantized_scalar_const_node("/b:0", scalar_input, op)
         self.add_min_max_const_node(op, op.input[0])
         self.add_min_max_const_node(op, op.input[1])
 
-        element_type = ConverterUtil.get_arg(
-            op, MaceKeyword.mace_element_type_str).i
         if element_type in [EltwiseType.SUM.value,
                             EltwiseType.SUB.value,
                             EltwiseType.MIN.value,
-                            EltwiseType.MAX.value]:
+                            EltwiseType.MAX.value,
+                            EltwiseType.DIV.value]:
             self.add_min_max_const_node(
                 op, op.output[0], True, True, False)
         try:
@@ -626,6 +761,9 @@ class HexagonConverter(base_converter.ConverterInterface):
         self.add_arg_const_node(
             op, '/strides:0', [1, strides_arg.ints[0], strides_arg.ints[1], 1])
 
+        self.add_padding_type_for_conv_pooling(
+            op, window_arg.ints, strides_arg.ints)
+
         pooling_type_arg = ConverterUtil.get_arg(
             op, MaceKeyword.mace_pooling_type_str)
         if PoolingType(pooling_type_arg.i) == PoolingType.AVG:
@@ -652,7 +790,7 @@ class HexagonConverter(base_converter.ConverterInterface):
         for i in axis_arg.ints:
             mace_check(1 <= i <= 2,
                        "Hexagon Reduce Mean only supports spatial now")
-        input_shape = get_input_shape(op.input[0], self._model)
+        input_shape = self.get_input_shape(op.input[0])
         if len(axis_arg.ints) == 1:
             dim1, dim2 = (input_shape[1], 1) \
                 if axis_arg.ints[0] == 1 else (1, input_shape[2])
@@ -664,10 +802,30 @@ class HexagonConverter(base_converter.ConverterInterface):
         op.type = HexagonOp.QuantizedAvgPool_8.name
 
     def convert_resizebilinear(self, op):
-        newdim_arg = ConverterUtil.get_arg(
+        resize_size_arg = ConverterUtil.get_arg(
             op, MaceKeyword.mace_resize_size_str)
-        self.add_arg_const_node(
-            op, '/newdim:0', [len(newdim_arg.ints)], newdim_arg.ints)
+        if resize_size_arg is not None:
+            newdim = resize_size_arg.ints
+        else:
+            height_scale_arg = ConverterUtil.get_arg(
+                op, MaceKeyword.mace_height_scale_str)
+            width_scale_arg = ConverterUtil.get_arg(
+                op, MaceKeyword.mace_width_scale_str)
+            mace_check(height_scale_arg is not None and
+                       width_scale_arg is not None,
+                       "Wrong ResizeBilinear arguments.")
+            if len(op.input) == 2:
+                op.input.pop()
+            height_scale = height_scale_arg.f
+            width_scale = width_scale_arg.f
+            producer_op = self._producers[op.input[0]]
+            for i in range(len(producer_op.output)):
+                if producer_op.output[i] == op.input[0]:
+                    input_shape = producer_op.output_shape[i]
+                    break
+            newdim = [int(height_scale * input_shape.dims[1]),
+                      int(width_scale * input_shape.dims[2])]
+        self.add_arg_const_node(op, '/newdim:0', [2], newdim)
 
         self.add_min_max_const_node(op, op.input[0])
 
@@ -678,11 +836,54 @@ class HexagonConverter(base_converter.ConverterInterface):
 
         op.type = HexagonOp.QuantizedResizeBilinear_8.name
 
+    def convert_resizenearestneighbor(self, op):
+        height_scale_arg = ConverterUtil.get_arg(
+            op, MaceKeyword.mace_height_scale_str)
+        width_scale_arg = ConverterUtil.get_arg(
+            op, MaceKeyword.mace_width_scale_str)
+        if height_scale_arg is not None:
+            mace_check(width_scale_arg is not None,
+                       "height scale and width scale should be present at the same time.")  # noqa
+            if len(op.input) == 2:
+                op.input.pop()
+            height_scale = height_scale_arg.f
+            width_scale = width_scale_arg.f
+            producer_op = self._producers[op.input[0]]
+            for i in range(len(producer_op.output)):
+                if producer_op.output[i] == op.input[0]:
+                    input_shape = producer_op.output_shape[i]
+                    break
+            newdim = [int(height_scale * input_shape.dims[1]),
+                      int(width_scale * input_shape.dims[2])]
+            self.add_arg_const_node(op, '/newdim:0', [2], newdim)
+
+        self.add_min_max_const_node(op, op.input[0])
+
+        align_corners_arg = ConverterUtil.get_arg(
+            op, MaceKeyword.mace_align_corners_str)
+        self.add_arg_const_node(
+            op, '/align_corners:0', [1], [align_corners_arg.i])
+
+        op.type = HexagonOp.ResizeNearestNeighbor_8.name
+
     def convert_softmax(self, op):
         self.add_min_max_const_node(op, op.input[0])
 
         op.type = HexagonOp.QuantizedSoftmax_8.name
 
+    def convert_split(self, op):
+        op_input = op.input[0]
+        del op.input[:]
+        axis_value = ConverterUtil.get_arg(op, MaceKeyword.mace_axis_str).i
+        self.add_arg_const_node(op, '/axis:0', [1], [axis_value])
+        op.input.append(op_input)
+        self.add_min_max_const_node(op, op_input)
+
+        for i in range(len(op.output) - 1):
+            op.output_type.append(mace_pb2.DT_UINT8)
+
+        op.type = HexagonOp.QuantizedSplit_8.name
+
     def convert_stridedslice(self, op):
         begin_mask = ConverterUtil.get_arg(
             op, MaceKeyword.mace_begin_mask_str).i
diff --git a/tools/python/transform/onnx_converter.py b/tools/python/transform/onnx_converter.py
index 4a4662af2453d89dc710e3646251cffdf9bb03a2..ef4023efe8239ae12a66a7ff28027cfcb7cf18b9 100644
--- a/tools/python/transform/onnx_converter.py
+++ b/tools/python/transform/onnx_converter.py
@@ -249,7 +249,7 @@ class OnnxNode(object):
     def __init__(self, node):
         self.name = str(node.name)
         if self.name == '':
-            self.name = str(node.output)
+            self.name = str('_'.join(node.output))
         self.op_type = str(node.op_type)
         self.domain = str(node.domain)
         self.attrs = dict([(attr.name,
@@ -567,8 +567,8 @@ class OnnxConverter(base_converter.ConverterInterface):
         for n in graph_def.node:
             node = OnnxNode(n)
             mace_check(node.op_type in self._op_converters,
-                       "Mace does not support onnx op type %s yet"
-                       % node.op_type)
+                       "Mace does not support onnx op type %s(%s) yet"
+                       % (node.name, node.op_type))
             self._op_converters[node.op_type](node)
 
     def convert_tensors(self, graph_def):
diff --git a/tools/python/transform/transformer.py b/tools/python/transform/transformer.py
index ff9788dfc409d83874a530a8b6809afc33b38d1d..5d77ba98e2ad7c3389a7ac31fdbb13baa1ee1dbd 100644
--- a/tools/python/transform/transformer.py
+++ b/tools/python/transform/transformer.py
@@ -1176,22 +1176,40 @@ class Transformer(base_converter.ConverterInterface):
         elif self._option.quantize and \
                 (self._option.device == DeviceType.HEXAGON.value or
                  self._option.device == DeviceType.HTA.value):
+            print("Transpose filters to HWIO/HWIM")
             for op in net.op:
-                # from HWOI to OHWI, deconv is unique
+                if filter_format == DataFormat.OIHW and \
+                        (op.type == MaceOp.Conv2D.name or
+                         op.type == MaceOp.DepthwiseConv2d.name or
+                         (op.type == MaceOp.FullyConnected.name and
+                          len(self._consts[op.input[1]].dims) == 4)) and \
+                        op.input[1] in self._consts and \
+                        op.input[1] not in transposed_filter:
+                    filter = self._consts[op.input[1]]
+                    filter_data = np.array(filter.float_data).reshape(
+                        filter.dims)
+                    filter_data = filter_data.transpose(2, 3, 1, 0)
+                    filter.float_data[:] = filter_data.flat
+                    filter.dims[:] = filter_data.shape
+                    transposed_filter.add(op.input[1])
+
                 if op.type == MaceOp.Deconv2D.name \
                         and op.input[1] in self._consts \
                         and op.input[1] not in transposed_deconv_filter:
                     filter = self._consts[op.input[1]]
                     filter_data = np.array(filter.float_data).reshape(
                         filter.dims)
-                    filter_data = filter_data.transpose(2, 0, 1, 3)
+                    if filter_format == DataFormat.HWIO:
+                        # from HWOI to OHWI
+                        filter_data = filter_data.transpose(2, 0, 1, 3)
+                    elif filter_format == DataFormat.OIHW:
+                        # from IOHW to OHWI
+                        filter_data = filter_data.transpose(1, 2, 3, 0)
+                    else:
+                        mace_check(False, "Unsupported filter format.")
                     filter.float_data[:] = filter_data.flat
                     filter.dims[:] = filter_data.shape
                     transposed_deconv_filter.add(op.input[1])
-
-            print("Transpose filters to HWIO/HWIM")
-            mace_check(filter_format == DataFormat.HWIO,
-                       "HEXAGON only support HWIO/HWIM filter format.")
         else:
             # transpose filter to OIHW/MIHW for tensorflow (HWIO/HWIM)
             if filter_format == DataFormat.HWIO:
@@ -1882,7 +1900,8 @@ class Transformer(base_converter.ConverterInterface):
             ops = self._consumers.get(tensor.name, None)
             if ops is not None and len(ops) == 1:
                 if ops[0].type in [MaceOp.Conv2D.name,
-                                   MaceOp.FullyConnected.name]:
+                                   MaceOp.FullyConnected.name,
+                                   MaceOp.MatMul.name]:
                     quantized_tensor = \
                         quantize_util.quantize(tensor.float_data,
                                                self._option.device,
diff --git a/tools/python/utils/device.py b/tools/python/utils/device.py
index 4c0fe1468514a89d27ea229fbbd21db4cad3da34..b9dc621b9098646cbe9006a1c575dc7275cefa98 100644
--- a/tools/python/utils/device.py
+++ b/tools/python/utils/device.py
@@ -173,22 +173,22 @@ class AndroidDevice(Device):
 
     def install_common_libs_for_target(self, target, install_dir):
         sn = self._device_id
-        dep_so_libs = execute(os.environ["ANDROID_NDK_HOME"] + "/ndk-depends "
-                              + target.path)
-        lib_file = ""
-        for dep in dep_so_libs.split("\n"):
-            if dep == "libgnustl_shared.so":
-                lib_file = "%s/sources/cxx-stl/gnu-libstdc++/4.9/libs/" \
-                           "%s/libgnustl_shared.so" \
-                           % (os.environ["ANDROID_NDK_HOME"], self._target_abi)
-            elif dep == "libc++_shared.so":
-                lib_file = "%s/sources/cxx-stl/llvm-libc++/libs/" \
-                           "%s/libc++_shared.so" \
-                           % (os.environ["ANDROID_NDK_HOME"], self._target_abi)
-
-        if lib_file:
-            execute("adb -s %s push %s %s" % (sn, lib_file, install_dir),
-                    False)
+        abi = self._target_abi
+        lib_file = "%s/sources/cxx-stl/llvm-libc++/libs/" \
+                   "%s/libc++_shared.so" \
+                   % (os.environ["ANDROID_NDK_HOME"], abi)
+        ndk_depends_path = os.environ["ANDROID_NDK_HOME"] + "/ndk-depends"
+        if os.path.exists(ndk_depends_path):
+            dep_so_libs = execute(ndk_depends_path + " " + target.path)
+            for dep in dep_so_libs.split("\n"):
+                if dep == "libgnustl_shared.so":
+                    lib_file = "%s/sources/cxx-stl/gnu-libstdc++/4.9/libs/" \
+                               "%s/libgnustl_shared.so" \
+                               % (os.environ["ANDROID_NDK_HOME"], abi)
+        else:
+            print("Find no ndk-depends, use default libc++_shared.so")
+
+        execute("adb -s %s push %s %s" % (sn, lib_file, install_dir), False)
 
     def run(self, target):
         tmpdirname = tempfile.mkdtemp()
